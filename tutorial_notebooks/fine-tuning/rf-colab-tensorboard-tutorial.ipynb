{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tVCZ8hwnWeLi"
      },
      "source": [
        "<div align=\"center\">\n",
        "<a href=\"https://rapidfire.ai/\"><img src=\"https://raw.githubusercontent.com/RapidFireAI/rapidfireai/main/docs/images/RapidFire - Blue bug -white text.svg\" width=\"115\"></a>\n",
        "<a href=\"https://discord.gg/6vSTtncKNN\"><img src=\"https://raw.githubusercontent.com/RapidFireAI/rapidfireai/main/docs/images/discord-button.svg\" width=\"145\"></a>\n",
        "<a href=\"https://oss-docs.rapidfire.ai/\"><img src=\"https://raw.githubusercontent.com/RapidFireAI/rapidfireai/main/docs/images/documentation-button.svg\" width=\"125\"></a>\n",
        "<br/>\n",
        "Join Discord if you need help + ‚≠ê <i>Star us on <a href=\"https://github.com/RapidFireAI/rapidfireai\">GitHub</a></i> ‚≠ê\n",
        "<br/>\n",
        "üëâ <b>Note:</b> This Colab notebook illustrates simplified usage of <code>rapidfireai</code>. For the full RapidFire AI experience with advanced experiment manager, UI, and production features, see our <a href=\\\"https://oss-docs.rapidfire.ai/en/latest/walkthrough.html\\\">Install and Get Started</a> guide.\n",
        "<br/>\n",
        "üé¨ Watch our <a href=\\\"https://youtu.be/nPMBfZWqPWI\\\">intro video</a> to get started!\\n\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J3DmTK3ZWeLp"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/RapidFireAI/rapidfireai/blob/main/tutorial_notebooks/fine-tuning/rf-colab-tensorboard-tutorial.ipynb)\n",
        "\n",
        "‚ö†Ô∏è **IMPORTANT:** Do not let the Colab notebook tab stay idle for more than 5min; Colab will disconnect otherwise. Refresh the TensorBoard screen or interact with the cells to avoid disconnection."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uHIblXRsAxJh"
      },
      "source": [
        "## Optimize Your Fine-tuning with RapidFire AI\n",
        "\n",
        "This tutorial demonstrates how to fine-tune LLMs using **Supervised Fine-Tuning (SFT)** with [RapidFire AI](https://github.com/RapidFireAI/rapidfireai), enabling you to train and compare multiple configurations concurrently‚Äîeven on a single GPU. We'll fine-tune a model on customer support data and explore how RapidFire AI's chunk-based scheduling delivers **16-24√ó faster experimentation throughput**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BBsjFNtu26ig"
      },
      "source": [
        "## Install RapidFire AI Package and Services"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "5tJM-VCw26ig",
        "outputId": "df4c0475-3498-41ea-cb1c-ad89068f1c99",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting rapidfireai\n",
            "  Downloading rapidfireai-0.12.8-py3-none-any.whl.metadata (24 kB)\n",
            "Requirement already satisfied: flask>=3.1.1 in /usr/local/lib/python3.12/dist-packages (from rapidfireai) (3.1.2)\n",
            "Collecting flask-cors>=6.0.1 (from rapidfireai)\n",
            "  Downloading flask_cors-6.0.2-py3-none-any.whl.metadata (5.3 kB)\n",
            "Collecting waitress>=3.0.2 (from rapidfireai)\n",
            "  Downloading waitress-3.0.2-py3-none-any.whl.metadata (5.8 kB)\n",
            "Collecting jq>=1.10.0 (from rapidfireai)\n",
            "  Downloading jq-1.10.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.0 kB)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from rapidfireai) (0.3.8)\n",
            "Collecting jedi>=0.16 (from rapidfireai)\n",
            "  Downloading jedi-0.19.2-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Collecting uv>=0.8.14 (from rapidfireai)\n",
            "  Downloading uv-0.9.22-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
            "Requirement already satisfied: blinker>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from flask>=3.1.1->rapidfireai) (1.9.0)\n",
            "Requirement already satisfied: click>=8.1.3 in /usr/local/lib/python3.12/dist-packages (from flask>=3.1.1->rapidfireai) (8.3.1)\n",
            "Requirement already satisfied: itsdangerous>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from flask>=3.1.1->rapidfireai) (2.2.0)\n",
            "Requirement already satisfied: jinja2>=3.1.2 in /usr/local/lib/python3.12/dist-packages (from flask>=3.1.1->rapidfireai) (3.1.6)\n",
            "Requirement already satisfied: markupsafe>=2.1.1 in /usr/local/lib/python3.12/dist-packages (from flask>=3.1.1->rapidfireai) (3.0.3)\n",
            "Requirement already satisfied: werkzeug>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from flask>=3.1.1->rapidfireai) (3.1.4)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.12/dist-packages (from jedi>=0.16->rapidfireai) (0.8.5)\n",
            "Downloading rapidfireai-0.12.8-py3-none-any.whl (46.4 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m46.4/46.4 MB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading flask_cors-6.0.2-py3-none-any.whl (13 kB)\n",
            "Downloading jedi-0.19.2-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m33.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jq-1.10.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (757 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m757.1/757.1 kB\u001b[0m \u001b[31m36.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading uv-0.9.22-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (22.2 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m22.2/22.2 MB\u001b[0m \u001b[31m44.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading waitress-3.0.2-py3-none-any.whl (56 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m56.2/56.2 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: waitress, uv, jq, jedi, flask-cors, rapidfireai\n",
            "Successfully installed flask-cors-6.0.2 jedi-0.19.2 jq-1.10.0 rapidfireai-0.12.8 uv-0.9.22 waitress-3.0.2\n",
            "Created directory: /content/rapidfireai/logs\n",
            "üîß Initializing RapidFire AI project...\n",
            "------------------------------\n",
            "Initializing project...\n",
            "Colab environment detected, installing fit packages\n",
            "Installing packages from /usr/local/lib/python3.12/dist-packages/setup/fit/requirements-colab.txt...\n",
            "‚úÖ Successfully installed packages from /usr/local/lib/python3.12/dist-packages/setup/fit/requirements-colab.txt\n",
            "Getting tutorial notebooks...\n",
            "Copying tutorial notebooks from /usr/local/lib/python3.12/dist-packages/tutorial_notebooks to ./tutorial_notebooks...\n",
            "‚úÖ Successfully copied notebooks to ./tutorial_notebooks\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    import rapidfireai\n",
        "    print(\"‚úÖ rapidfireai already installed\")\n",
        "except ImportError:\n",
        "    %pip install rapidfireai  # Takes 1 min\n",
        "    !rapidfireai init # Takes 1 min"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sry0i-xm26ih"
      },
      "source": [
        "## Start RapidFire Services\n",
        "\n",
        "- If any issues arise, can check the status in a terminal window using `rapidfireai status` or `rapidfireai doctor`\n",
        "- **Note:** You can also run `rapidfireai start` from a terminal on Colab insted of the cell below.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xv4BAgKr26ih"
      },
      "outputs": [],
      "source": [
        "import subprocess\n",
        "from time import sleep\n",
        "import socket\n",
        "try:\n",
        "  s = [socket.socket(socket.AF_INET, socket.SOCK_STREAM), socket.socket(socket.AF_INET, socket.SOCK_STREAM), socket.socket(socket.AF_INET, socket.SOCK_STREAM)]\n",
        "  s[0].connect((\"127.0.0.1\", 8851))\n",
        "  s[1].connect((\"127.0.0.1\", 8852))\n",
        "  s[2].connect((\"127.0.0.1\", 8853))\n",
        "  s[0].close()\n",
        "  s[1].close()\n",
        "  s[2].close()\n",
        "  print(\"RapidFire Services are running\")\n",
        "except OSError as error:\n",
        "  print(\"RapidFire Services are not running, launching now...\")\n",
        "  subprocess.Popen([\"rapidfireai\", \"start\"])\n",
        "  sleep(30)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4v66VLdtAxJj"
      },
      "source": [
        "## Configure RapidFire to Use TensorBoard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wbo1EcUmAxJj"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# Load TensorBoard extension\n",
        "%load_ext tensorboard\n",
        "\n",
        "# Configure RapidFire to use TensorBoard\n",
        "os.environ['RF_TRACKING_BACKEND'] = 'tensorboard'  # Options: 'mlflow', 'tensorboard', 'both'\n",
        "# TensorBoard log directory will be auto-created in experiment path"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pgK5ZQFnAxJj"
      },
      "source": [
        "## Import RapidFire Components"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OAde0aIfAxJk"
      },
      "outputs": [],
      "source": [
        "from rapidfireai import Experiment\n",
        "from rapidfireai.fit.automl import List, RFGridSearch, RFModelConfig, RFLoraConfig, RFSFTConfig\n",
        "\n",
        "# NB: If you get \"AttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\" from Colab, just rerun this cell"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "adszyLwxAxJk"
      },
      "source": [
        "## Load Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vil1zbTeAxJk"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"bitext/Bitext-customer-support-llm-chatbot-training-dataset\")\n",
        "\n",
        "# REDUCED dataset for memory constraints in Colab\n",
        "train_dataset = dataset[\"train\"].select(range(64))  # Reduced from 128\n",
        "eval_dataset = dataset[\"train\"].select(range(50, 60))  # 10 examples\n",
        "train_dataset = train_dataset.shuffle(seed=42)\n",
        "eval_dataset = eval_dataset.shuffle(seed=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i3iorhjgAxJk"
      },
      "source": [
        "## Define Data Processing Function\n",
        "\n",
        "We'll format the data as Q&A pairs for GPT-2:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gpnc3duXAxJk"
      },
      "outputs": [],
      "source": [
        "def sample_formatting_function(example):\n",
        "    \"\"\"Format the dataset for GPT-2 while preserving original fields\"\"\"\n",
        "    return {\n",
        "        \"text\": f\"Question: {example['instruction']}\\nAnswer: {example['response']}\",\n",
        "        \"instruction\": example['instruction'],  # Keep original\n",
        "        \"response\": example['response']  # Keep original\n",
        "    }\n",
        "\n",
        "# Apply formatting to datasets\n",
        "eval_dataset = eval_dataset.map(sample_formatting_function)\n",
        "train_dataset = train_dataset.map(sample_formatting_function)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "--lWb0qnAxJk"
      },
      "source": [
        "## Define Metrics Function\n",
        "\n",
        "We'll use a lightweight metrics computation with just ROUGE-L to save memory:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Gqa6JduAxJk"
      },
      "outputs": [],
      "source": [
        "def sample_compute_metrics(eval_preds):\n",
        "    \"\"\"Lightweight metrics computation\"\"\"\n",
        "    predictions, labels = eval_preds\n",
        "\n",
        "    try:\n",
        "        import evaluate\n",
        "\n",
        "        # Only compute ROUGE-L (skip BLEU to save memory)\n",
        "        rouge = evaluate.load(\"rouge\")\n",
        "        rouge_output = rouge.compute(\n",
        "            predictions=predictions,\n",
        "            references=labels,\n",
        "            use_stemmer=True,\n",
        "            rouge_types=[\"rougeL\"]  # Only compute rougeL\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            \"rougeL\": round(rouge_output[\"rougeL\"], 4),\n",
        "        }\n",
        "    except Exception as e:\n",
        "        # Fallback if metrics fail\n",
        "        print(f\"Metrics computation failed: {e}\")\n",
        "        return {}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0zW2g7CJAxJk"
      },
      "source": [
        "## Initialize Experiment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZQ6mbRK6AxJl"
      },
      "outputs": [],
      "source": [
        "# Create experiment with unique name\n",
        "my_experiment = \"tensorboard-demo-1\"\n",
        "experiment = Experiment(experiment_name=my_experiment)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gAYPi61cAxJl"
      },
      "source": [
        "## Get TensorBoard Log Directory\n",
        "\n",
        "The TensorBoard logs are stored in the experiment directory. Let's get the path:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9QPR4y-YAxJl"
      },
      "outputs": [],
      "source": [
        "# Get experiment path\n",
        "from rapidfireai.fit.db.rf_db import RfDb\n",
        "\n",
        "db = RfDb()\n",
        "experiment_path = db.get_experiments_path(my_experiment)\n",
        "tensorboard_log_dir = f\"{experiment_path}/{my_experiment}/tensorboard_logs\"\n",
        "\n",
        "print(f\"TensorBoard logs will be saved to: {tensorboard_log_dir}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pALJJyZcAxJl"
      },
      "source": [
        "## Define Model Configurations\n",
        "\n",
        "This tutorial showcases GPT-2 (124M parameters), which is perfect for Colab's memory constraints:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "shEQVD7kAxJl"
      },
      "outputs": [],
      "source": [
        "# GPT-2 specific LoRA configs - different module names!\n",
        "peft_configs_lite = List([\n",
        "    RFLoraConfig(\n",
        "        r=8,\n",
        "        lora_alpha=16,\n",
        "        lora_dropout=0.1,\n",
        "        target_modules=[\"c_attn\"],  # GPT-2 combines Q,K,V in c_attn\n",
        "        bias=\"none\"\n",
        "    ),\n",
        "    RFLoraConfig(\n",
        "        r=32,\n",
        "        lora_alpha=64,\n",
        "        lora_dropout=0.1,\n",
        "        target_modules=[\"c_attn\", \"c_proj\"],  # c_attn (QKV) + c_proj (output)\n",
        "        bias=\"none\"\n",
        "    )\n",
        "])\n",
        "\n",
        "# 2 configs with GPT-2\n",
        "config_set_lite = List([\n",
        "    RFModelConfig(\n",
        "        model_name=\"gpt2\",  # Only 124M params\n",
        "        peft_config=peft_configs_lite,\n",
        "        training_args=RFSFTConfig(\n",
        "            learning_rate=5e-4,  # Low lr for more stability\n",
        "            lr_scheduler_type=\"linear\",\n",
        "            per_device_train_batch_size=2,\n",
        "            gradient_accumulation_steps=2,  # Effective bs = 4\n",
        "            max_steps=64, # Raise this to see more learning\n",
        "            logging_steps=2,\n",
        "            eval_strategy=\"steps\",\n",
        "            eval_steps=4,\n",
        "            per_device_eval_batch_size=2,\n",
        "            fp16=True,\n",
        "            gradient_checkpointing=True,  # Save memory\n",
        "            report_to=\"none\",  # Disables wandb\n",
        "        ),\n",
        "        model_type=\"causal_lm\",\n",
        "        model_kwargs={\n",
        "            \"device_map\": \"auto\",\n",
        "            \"torch_dtype\": \"float16\",  # Explicit fp16\n",
        "            \"use_cache\": False\n",
        "        },\n",
        "        formatting_func=sample_formatting_function,\n",
        "        compute_metrics=sample_compute_metrics,\n",
        "        generation_config={\n",
        "            \"max_new_tokens\": 128,  # Reduced from 256\n",
        "            \"temperature\": 0.7,\n",
        "            \"top_p\": 0.9,\n",
        "            \"top_k\": 40,\n",
        "            \"repetition_penalty\": 1.1,\n",
        "            \"pad_token_id\": 50256,  # GPT-2's EOS token\n",
        "        }\n",
        "    ),\n",
        "    RFModelConfig(\n",
        "        model_name=\"gpt2\",\n",
        "        peft_config=peft_configs_lite,\n",
        "        training_args=RFSFTConfig(\n",
        "            learning_rate=2e-4,  # Even more conservative\n",
        "            lr_scheduler_type=\"cosine\",  # Try cosine schedule\n",
        "            per_device_train_batch_size=2,\n",
        "            gradient_accumulation_steps=2,\n",
        "            max_steps=64, # Raise this to see more learning behviors\n",
        "            logging_steps=2,\n",
        "            eval_strategy=\"steps\",\n",
        "            eval_steps=4,\n",
        "            per_device_eval_batch_size=2,\n",
        "            fp16=True,\n",
        "            gradient_checkpointing=True,\n",
        "            report_to=\"none\",  # Disables wandb\n",
        "            warmup_steps=10,  # Add warmup for stability\n",
        "        ),\n",
        "        model_type=\"causal_lm\",\n",
        "        model_kwargs={\n",
        "            \"device_map\": \"auto\",\n",
        "            \"torch_dtype\": \"float16\",\n",
        "            \"use_cache\": False\n",
        "        },\n",
        "        formatting_func=sample_formatting_function,\n",
        "        compute_metrics=sample_compute_metrics,\n",
        "        generation_config={\n",
        "            \"max_new_tokens\": 128,\n",
        "            \"temperature\": 0.7,\n",
        "            \"top_p\": 0.9,\n",
        "            \"top_k\": 40,\n",
        "            \"repetition_penalty\": 1.1,\n",
        "            \"pad_token_id\": 50256,\n",
        "        }\n",
        "    )\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wuo9B8WrAxJl"
      },
      "outputs": [],
      "source": [
        "def sample_create_model(model_config):\n",
        "    \"\"\"Function to create model object with GPT-2 adjustments\"\"\"\n",
        "    from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "    model_name = model_config[\"model_name\"]\n",
        "    model_type = model_config[\"model_type\"]\n",
        "    model_kwargs = model_config[\"model_kwargs\"]\n",
        "\n",
        "    if model_type == \"causal_lm\":\n",
        "        model = AutoModelForCausalLM.from_pretrained(model_name, **model_kwargs)\n",
        "    else:\n",
        "        # Default to causal LM\n",
        "        model = AutoModelForCausalLM.from_pretrained(model_name, **model_kwargs)\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "    # GPT-2 specific: Set pad token (GPT-2 doesn't have one by default)\n",
        "    if \"gpt2\" in model_name.lower():\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "        tokenizer.padding_side = \"left\"  # GPT-2 works better with left padding\n",
        "        model.config.pad_token_id = model.config.eos_token_id\n",
        "\n",
        "    return (model, tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v7NOeq0PAxJl"
      },
      "outputs": [],
      "source": [
        "# Simple grid search across all config combinations: 4 total (2 LoRA configs √ó 2 trainer configs)\n",
        "config_group = RFGridSearch(\n",
        "    configs=config_set_lite,\n",
        "    trainer_type=\"SFT\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wJfXimPpAxJl"
      },
      "source": [
        "## Start TensorBoard\n",
        "\n",
        "**IMPORTANT: Make sure to start TensorBoard BEFORE invoking run_fit() below so that you can watch metrics appear in real-time!**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qVVWU42vKBTN"
      },
      "outputs": [],
      "source": [
        "%tensorboard --logdir {tensorboard_log_dir}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oGOs_rZYAxJm"
      },
      "source": [
        "## Run Training + Validation\n",
        "\n",
        "Now we get to the main function for running multi-config training and evals. The metrics will appear in TensorBoard above in real-time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AykcHp33AxJm"
      },
      "outputs": [],
      "source": [
        "# Launch train and validation for all configs in the config_group with swap granularity of 4 chunks for hyperparallel execution\n",
        "experiment.run_fit(\n",
        "    config_group,\n",
        "    sample_create_model,\n",
        "    train_dataset,\n",
        "    eval_dataset,\n",
        "    num_chunks=4,\n",
        "    seed=42\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gjmG-lC526ij"
      },
      "source": [
        "## Launch Interactive Run Controller\n",
        "\n",
        "RapidFire AI provides an Interactive Controller that lets you manage executing runs dynamically in real-time from the notebook:\n",
        "\n",
        "- ‚èπÔ∏è **Stop**: Gracefully stop a running config\n",
        "- ‚ñ∂Ô∏è **Resume**: Resume a stopped run\n",
        "- üóëÔ∏è **Delete**: Remove a run from this experiment\n",
        "- üìã **Clone**: Create a new run by editing the config dictionary of a parent run to try new knob values; optional warm start of parameters\n",
        "- üîÑ **Refresh**: Update run status and metrics\n",
        "\n",
        "The Controller uses ipywidgets and is compatible with both Colab (ipywidgets 7.x) and Jupyter (ipywidgets 8.x)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JMuqPmQT26ij"
      },
      "outputs": [],
      "source": [
        "# Create Interactive Controller\n",
        "sleep(15)\n",
        "from rapidfireai.fit.utils.interactive_controller import InteractiveController\n",
        "\n",
        "controller = InteractiveController(dispatcher_url=\"http://127.0.0.1:8851\")\n",
        "controller.display()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ujpATTaRAxJm"
      },
      "source": [
        "## End Experiment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uOTTI-rVAxJm"
      },
      "outputs": [],
      "source": [
        "from google.colab import output\n",
        "from IPython.display import display, HTML\n",
        "\n",
        "display(HTML('''\n",
        "<button id=\"continue-btn\" style=\"padding: 10px 20px; font-size: 16px;\">Click to End Experiment</button>\n",
        "'''))\n",
        "\n",
        "# eval_js blocks until the Promise resolves\n",
        "output.eval_js('''\n",
        "new Promise((resolve) => {\n",
        "    document.getElementById(\"continue-btn\").onclick = () => {\n",
        "        document.getElementById(\"continue-btn\").disabled = true;\n",
        "        document.getElementById(\"continue-btn\").innerText = \"Continuing...\";\n",
        "        resolve(\"clicked\");\n",
        "    };\n",
        "})\n",
        "''')\n",
        "\n",
        "# Actually end the experiment after the button is clicked\n",
        "experiment.end()\n",
        "print(\"Done!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wuiwNvldAxJm"
      },
      "source": [
        "## View TensorBoard Plots and Logs\n",
        "\n",
        "After your experiment is ended, you can still view the full logs in TensorBoard:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rvbsKwE2AxJm"
      },
      "outputs": [],
      "source": [
        "# View final logs\n",
        "%tensorboard --logdir {tensorboard_log_dir}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mXeBD1QjWeLv"
      },
      "source": [
        "## View RapidFire AI Log Files\n",
        "\n",
        "You can track the work being done by the system via the RapidFire AI-produced log files in logs/experiments/ folder."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "relSCdB226ij"
      },
      "outputs": [],
      "source": [
        "# Get the experiment-specific log file\n",
        "from IPython.display import display, Pretty\n",
        "log_file = experiment.get_log_file_path()\n",
        "\n",
        "display(Pretty(f\"üìÑ Experiment Log File: {log_file}\"))\n",
        "\n",
        "if log_file.exists():\n",
        "    display(Pretty(\"=\" * 80))\n",
        "    display(Pretty(f\"Last 30 lines of {log_file.name}:\"))\n",
        "    display(Pretty(\"=\" * 80))\n",
        "    with open(log_file, 'r', encoding='utf-8') as f:\n",
        "        lines = f.readlines()\n",
        "        for line in lines[-30:]:\n",
        "            display(Pretty(line.rstrip()))\n",
        "else:\n",
        "    display(Pretty(f\"‚ùå Log file not found: {log_file}\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1xMdpOe_26ij"
      },
      "outputs": [],
      "source": [
        "# Get the training-specific log file\n",
        "log_file = experiment.get_log_file_path(\"training\")\n",
        "\n",
        "display(Pretty(f\"üìÑ Training Log File: {log_file}\"))\n",
        "\n",
        "if log_file.exists():\n",
        "    display(Pretty(\"=\" * 80))\n",
        "    display(Pretty(f\"Last 30 lines of {log_file.name}:\"))\n",
        "    display(Pretty(\"=\" * 80))\n",
        "    with open(log_file, 'r', encoding='utf-8') as f:\n",
        "        lines = f.readlines()\n",
        "        for line in lines[-30:]:\n",
        "            display(Pretty(line.rstrip()))\n",
        "else:\n",
        "    display(Pretty(f\"‚ùå Log file not found: {log_file}\"))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}